# Focused Reading List

- Vision Transformer (ViT): Dosovitskiy et al., 2020 (image transformers).
- CLIP: Radford et al., 2021 (multimodal pretraining).
- DNABERT / DNABERT-2: Ji et al., 2021+ (language models for DNA).
- Enformer: Avsec et al., 2021 (long-range genomic transformer).
- Geneformer: Theodoris et al. (foundation model for single-cell expression at scale).
- Multimodal fusion strategies (early/intermediate/late) â€” recent surveys and applied works.
- Example imaging+genomics transformer approaches (e.g., cross-modal attention, joint embeddings).

Key takeaway: transformer-based embeddings enable flexible fusion; intermediate fusion or cross-attention often outperform unimodal baselines and naive averaging.
